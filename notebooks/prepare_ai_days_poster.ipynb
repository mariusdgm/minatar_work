{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "\n",
    "proj_root = os.path.dirname(os.path.abspath(\".\"))\n",
    "# print(proj_root)\n",
    "sys.path.append(proj_root)\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import gym \n",
    "import yaml\n",
    "\n",
    "from minatar_dqn.replay_buffer import ReplayBuffer\n",
    "from experiments.experiment_utils import seed_everything\n",
    "from minatar_dqn.utils.my_logging import setup_logger\n",
    "from minatar_dqn.models import Conv_QNET, Conv_QNET_one\n",
    "\n",
    "from minatar_dqn.my_dqn import Conv_QNET, build_environment\n",
    "from minatar_dqn.redo import apply_redo_parametrization\n",
    "from experiments.experiment_utils import collect_training_output_files, collect_pruning_output_files\n",
    "\n",
    "from flatten_dict import flatten\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_timestamp_folder = \"2023_05_22-08_44_19\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_outputs_folder_path = (\n",
    "    r\"D:\\Work\\PhD\\minatar_work\\experiments\\training\\outputs\"\n",
    ")\n",
    "\n",
    "experiment_paths = collect_training_output_files(\n",
    "        os.path.join(training_outputs_folder_path, training_timestamp_folder)\n",
    "    )\n",
    "\n",
    "def get_records_from_training_experiment(experiment_files, stats_type = \"validation\"):\n",
    "    \n",
    "    checkpoint = torch.load(experiment_files[\"stats_path\"])\n",
    "    \n",
    "    training_stats = checkpoint[\"training_stats\"]\n",
    "    validation_stats = checkpoint[\"validation_stats\"]\n",
    "\n",
    "    if stats_type == \"training\":\n",
    "        flat_records = process_records_from_stats_and_config(training_stats, experiment_files[\"config_path\"], stats_type)\n",
    "    elif stats_type == \"validation\":\n",
    "        flat_records = process_records_from_stats_and_config(validation_stats, experiment_files[\"config_path\"], stats_type)\n",
    "    \n",
    "    return flat_records\n",
    "\n",
    "def process_records_from_stats_and_config(stats, config_path, stats_type):\n",
    "\n",
    "    flat_records = []\n",
    "    for epoch_stats in stats:\n",
    "        flat_stats = flatten(epoch_stats, reducer=\"underscore\")  \n",
    "        flat_stats[\"epoch_type\"] = stats_type\n",
    "        flat_records.append(flat_stats)\n",
    "\n",
    "    with open(config_path, \"r\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "\n",
    "    # build experiment name cause I did not think to save some kind of \n",
    "    # exp name in config\n",
    "    config_file_name = os.path.basename(config_path)\n",
    "\n",
    "    # add info from the config file\n",
    "    for record in flat_records:\n",
    "        record[\"environment\"] = config[\"environment\"]\n",
    "        record[\"seed\"] = config[\"seed\"]\n",
    "        record[\"experiment_name\"] = config[\"experiment_name\"]\n",
    "        \n",
    "    return flat_records\n",
    "\n",
    "def collect_training_experiment_results(experiment_paths, stats=\"validation\", train_epoch_counter = 200_000):\n",
    "    records = []\n",
    "    for experiment_files in experiment_paths:\n",
    "        if experiment_files: # in case experiment did not finish\n",
    "            records.extend(get_records_from_training_experiment(experiment_files, stats_type=stats))\n",
    "\n",
    "    df = pd.DataFrame.from_records(records)\n",
    "    df[\"epoch\"] = df[\"frame_stamp\"] // train_epoch_counter \n",
    "\n",
    "    return df\n",
    "\n",
    "df = collect_training_experiment_results(experiment_paths, stats=\"validation\")\n",
    "# df = collect_training_experiment_results(experiment_paths, stats=\"training\")\n",
    "\n",
    "print(df.shape)\n",
    "df.head()\n",
    "\n",
    "df[\"model_name\"] = df[\"experiment_name\"].map(lambda x: \"_\".join(x.split(\"_\")[0:2]))\n",
    "df[\"model_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MARK PLOT FOR POSTER\n",
    "\n",
    "model_order = [\"conv8_lin32\", \"conv16_lin64\", \"conv32_lin128\", \"conv64_lin256\"]\n",
    "g = sns.FacetGrid(df, row='environment', col='model_name', height=4, aspect=1.5, sharey=\"row\", col_order=model_order)\n",
    "\n",
    "g.map(sns.lineplot, 'epoch', 'episode_rewards_mean')\n",
    "\n",
    "g.set_titles('Experiment: {col_name}, Environment: {row_name}')\n",
    "g.set_xlabels('Epoch')\n",
    "g.set_ylabels('Reward')\n",
    "\n",
    "# Display the legend\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "g.fig.legend(handles, labels, loc='upper right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruning_outputs_folder_path = (\n",
    "    r\"D:\\Work\\PhD\\minatar_work\\experiments\\pruning\\outputs\"\n",
    ")\n",
    "pruning_timestamp_folder = training_timestamp_folder\n",
    "\n",
    "experiment_paths = collect_pruning_output_files(\n",
    "        os.path.join(pruning_outputs_folder_path, pruning_timestamp_folder)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pruning_stats_file(file):\n",
    "\n",
    "    checkpoint = torch.load(file)\n",
    "\n",
    "    exp_stats = checkpoint[\"pruning_validation_results\"]\n",
    "    exp_info = checkpoint[\"experiment_info\"]\n",
    "\n",
    "    records = []\n",
    "    for pruning_value in exp_stats:\n",
    "        stats = exp_stats[pruning_value]\n",
    "        stats[\"pruning_value\"] = pruning_value\n",
    "        stats[\"pruning_method\"] = os.path.basename(file).split(\"_\")[-1]\n",
    "        stats[\"experiment_info\"] = exp_info\n",
    "        records.append(stats)\n",
    "\n",
    "    return records\n",
    "\n",
    "def add_baseline_stats(baseline_file, file):\n",
    "    \"\"\"Add a new record with the baseline stats and an associated \n",
    "    pruning_method just to make data aggregation easier.\"\"\"\n",
    "    checkpoint = torch.load(baseline_file)\n",
    "    baseline_stats = checkpoint[\"pruning_validation_results\"]\n",
    "    baseline_info = checkpoint[\"experiment_info\"]\n",
    "\n",
    "    records = []\n",
    "    for pruning_value in baseline_stats:\n",
    "        stats = baseline_stats[pruning_value]\n",
    "        stats[\"pruning_value\"] = pruning_value\n",
    "        stats[\"pruning_method\"] = os.path.basename(file).split(\"_\")[-1]\n",
    "        stats[\"experiment_info\"] = baseline_info\n",
    "\n",
    "        records.append(stats)\n",
    "        \n",
    "    return records\n",
    "\n",
    "def get_records_from_pruning_experiment(experiment_files):\n",
    "\n",
    "    config_path = experiment_files[\"config_path\"]\n",
    "\n",
    "    stats = []\n",
    "    for file in experiment_files[\"pruning_stats_paths\"]:\n",
    "        stats.extend(process_pruning_stats_file(file))\n",
    "        stats.extend(add_baseline_stats(experiment_files[\"baseline_stats_path\"], file))\n",
    "    \n",
    "    flat_records = []\n",
    "    for epoch_stats in stats:\n",
    "        flat_stats = flatten(epoch_stats, reducer=\"underscore\")  \n",
    "        flat_records.append(flat_stats)\n",
    "\n",
    "    with open(config_path, \"r\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "\n",
    "    # build experiment name cause I did not think to save some kind of \n",
    "    # exp name in config\n",
    "    config_file_name = os.path.basename(config_path)\n",
    "    exp_name_tokens = config_file_name.split(\"_\")[:2]\n",
    "    exp_name = \"_\".join(exp_name_tokens)\n",
    "\n",
    "    # add info from the config file\n",
    "    for record in flat_records:\n",
    "        record[\"environment\"] = config[\"environment\"]\n",
    "        record[\"seed\"] = config[\"seed\"]\n",
    "        record[\"model_name\"] = exp_name\n",
    "\n",
    "    return flat_records\n",
    "    \n",
    "def collect_pruning_experiment_results(experiment_paths):\n",
    "\n",
    "    records = []\n",
    "    for experiment_files in experiment_paths:\n",
    "        records.extend(get_records_from_pruning_experiment(experiment_files))\n",
    "\n",
    "    df = pd.DataFrame.from_records(records)\n",
    "    # df[\"epoch\"] = df[\"frame_stamp\"] // train_epoch_counter \n",
    "\n",
    "    return df\n",
    "\n",
    "df = collect_pruning_experiment_results(experiment_paths)\n",
    "\n",
    "pruning_method_nr_to_name = {\n",
    "    '1': 'prune_middle',\n",
    "    '2': 'prune_features',\n",
    "    '3': 'prune_all',\n",
    "    '4': 's_prune_conv'\n",
    "}\n",
    "\n",
    "# Map the 'fruit' column to 'color'\n",
    "df['pruning_method'] = df['pruning_method'].map(pruning_method_nr_to_name)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_df_using_baseline_single_env(df):\n",
    "    \n",
    "    cols_to_not_normalize = ['frame_stamp', 'epoch_time', 'pruning_value', 'pruning_method',\n",
    "       'experiment_info', 'environment', 'seed', 'model_name']\n",
    "\n",
    "    # Find the baseline values\n",
    "    baseline = df[(df[\"pruning_value\"] == 0)]\n",
    "\n",
    "    groups = df.groupby([\"model_name\", \"seed\"])\n",
    "\n",
    "    for (model_name, seed), group in groups:\n",
    "    \n",
    "        for metric in group.columns.difference(cols_to_not_normalize):\n",
    "\n",
    "            # Get the baseline value for this metric and model/seed/pruning_method combination\n",
    "            baseline_value = baseline[(baseline[\"model_name\"] == model_name) & (baseline[\"seed\"] == seed)][metric].iloc[0]\n",
    "        \n",
    "            # Normalize the values for this metric by the baseline value\n",
    "            group[metric] /= baseline_value\n",
    "            \n",
    "            # Store the normalized values back in the original DataFrame\n",
    "            df.loc[group.index, metric] = group[metric]\n",
    "\n",
    "    return df\n",
    "\n",
    "def normalize_df_using_baseline_all_envs(df):\n",
    "    \n",
    "    cols_to_not_normalize = ['frame_stamp', 'epoch_time', 'pruning_value', 'pruning_method',\n",
    "       'experiment_info', 'environment', 'seed', 'model_name']\n",
    "\n",
    "    # Find the baseline values\n",
    "    baseline = df[(df[\"pruning_value\"] == 0)]\n",
    "\n",
    "    groups = df.groupby([\"environment\", \"model_name\", \"seed\"])\n",
    "\n",
    "    for (environment, model_name, seed), group in groups:\n",
    "    \n",
    "        for metric in group.columns.difference(cols_to_not_normalize):\n",
    "\n",
    "            # Get the baseline value for this metric and model/seed/pruning_method combination\n",
    "            baseline_value = baseline[(baseline[\"environment\"] == environment) & (baseline[\"model_name\"] == model_name) & (baseline[\"seed\"] == seed)][metric].iloc[0]\n",
    "        \n",
    "            # Normalize the values for this metric by the baseline value\n",
    "            group[metric] /= baseline_value\n",
    "            \n",
    "            # Store the normalized values back in the original DataFrame\n",
    "            df.loc[group.index, metric] = group[metric]\n",
    "\n",
    "    return df\n",
    "\n",
    "def plot_facetgrid_experiment(df, y_label = \"Mean score\", title=\"Mean score at different levels of pruning\"):\n",
    "\n",
    "    experiment_order = ['conv8_lin32', 'conv16_lin64', 'conv32_lin128', 'conv64_lin256']\n",
    "\n",
    "    # Compute the average metric between the seeds for each experiment\n",
    "    # avg_df = df.groupby(['environment', 'experiment_name', 'epoch']).mean().reset_index()\n",
    "\n",
    "    g = sns.FacetGrid(df, row='pruning_method', col='model_name', col_order=experiment_order, \n",
    "    height=3, aspect=1, sharey=\"row\")\n",
    "    g.map(sns.lineplot, 'pruning_value', 'episode_rewards_mean')\n",
    "\n",
    "    # Set the plot title and axis labels\n",
    "    g.fig.suptitle(title)\n",
    "    g.set_titles(row_template='{row_name}', col_template='{col_name}', fontsize=1)\n",
    "\n",
    "    g.figure.subplots_adjust(top=0.9, wspace=0.1)\n",
    "    g.set_axis_labels('Pruning factor', y_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MARK: needed for poster\n",
    "\n",
    "sub_df = df.copy(deep=True)\n",
    "sub_df = normalize_df_using_baseline_all_envs(sub_df)\n",
    "\n",
    "plot_facetgrid_experiment(sub_df, y_label = \"Normalized score\", title=f\"Normalized mean score at different levels of pruning (All envs aggregated)\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_to_score_distance(mask, scores):\n",
    "    num_ones = np.count_nonzero(mask)\n",
    "\n",
    "    if num_ones == 0:\n",
    "        return 0\n",
    "    \n",
    "    # Sort the scores in ascending order and get their corresponding indices\n",
    "    sorted_indices = np.argsort(scores)\n",
    "    \n",
    "    # Create a new mask where the N smallest values represent True\n",
    "    new_mask = torch.zeros_like(mask, dtype=torch.bool)\n",
    "    new_mask[sorted_indices[:num_ones]] = True\n",
    "     \n",
    "    # Calculate the Hamming distance between the masks\n",
    "    hamming_distance = torch.sum(torch.logical_and(mask, new_mask))\n",
    "    hamming_relative_distance = hamming_distance.item() / len(mask)\n",
    "\n",
    "    return hamming_relative_distance\n",
    "\n",
    "def mask_vs_norm_stats(mask, scores):\n",
    "    num_ones = np.count_nonzero(mask)\n",
    "    \n",
    "    # Sort the scores in ascending order and get their corresponding indices\n",
    "    sorted_indices = np.argsort(scores)\n",
    "    \n",
    "    # Create a new mask where the N smallest values represent True\n",
    "    new_mask = torch.zeros_like(mask, dtype=torch.bool)\n",
    "    new_mask[sorted_indices[:num_ones]] = True\n",
    "     \n",
    "    # Calculate the Hamming distance between the masks\n",
    "    only_redo = torch.sum(torch.logical_and(mask, torch.eq(new_mask, False))).item() \n",
    "    intersect = torch.sum(torch.logical_and(mask, new_mask)).item() \n",
    "    only_pruning = torch.sum(torch.logical_and(torch.eq(mask, False), new_mask)).item()\n",
    "\n",
    "    return only_redo, intersect, only_pruning\n",
    "\n",
    "def get_rankings(scores):\n",
    "    sorted_indices = torch.argsort(scores)\n",
    "\n",
    "    # Initialize tensor for rankings\n",
    "    rankings = torch.zeros_like(scores, dtype=torch.long)\n",
    "\n",
    "    # Assign ranks to scores\n",
    "    rankings[sorted_indices] = torch.arange(1, len(scores) + 1)\n",
    "\n",
    "    return rankings\n",
    "\n",
    "def compare_redo_pruning(exp_paths):\n",
    "\n",
    "    training_stats_data = torch.load(exp_paths[\"stats_path\"])\n",
    "    redo_scores = training_stats_data[\"redo_scores\"][\"policy\"]\n",
    "\n",
    "    checkpoints_paths = search_files_containing_string(\n",
    "        exp_paths[\"models_folder_path\"], \"mck\", substring_location=\"containing\"\n",
    "    )\n",
    "\n",
    "    # read the config so that the model architecture can be loaded\n",
    "    with open(exp_paths[\"config_path\"], \"r\") as ymlfile:\n",
    "        cfg = yaml.safe_load(ymlfile)\n",
    "\n",
    "    results = []\n",
    "    for checkpoint_path in checkpoints_paths:\n",
    "        \n",
    "        # Build a new model\n",
    "        validation_env = build_environment(\n",
    "            cfg[\"environment\"], cfg[\"seed\"]\n",
    "        )\n",
    "\n",
    "        # returns state as [w, h, channels]\n",
    "        state_shape = validation_env.observation_space.shape\n",
    "\n",
    "        # permute to get batch, channel, w, h shape\n",
    "        # specific to minatar\n",
    "        in_features = (state_shape[2], state_shape[0], state_shape[1])\n",
    "        in_channels = in_features[0]\n",
    "        num_actions = validation_env.action_space.n\n",
    "\n",
    "        checkpoint_model = Conv_QNET(in_features = in_features, \n",
    "                                    in_channels = in_channels, \n",
    "                                    num_actions = num_actions, \n",
    "                                    **cfg[\"estimator\"][\"args_\"])\n",
    "        checkpoint_model = apply_redo_parametrization(checkpoint_model, tau=0.1)\n",
    "\n",
    "        # Get the state of the model\n",
    "        checkpoint_models_state = torch.load(checkpoint_path)\n",
    "        checkpoint_model.load_state_dict(checkpoint_models_state[\"policy_model_state_dict\"])\n",
    "        model_state = checkpoint_model.state_dict()\n",
    "\n",
    "        # Compute kendall rank correlation coefficient for each layer\n",
    "        layer_weight_keys = [\"features.0.weight\", \"features.2.weight\", \"fc.0.weight\"]\n",
    "        for i, layer_key in enumerate(layer_weight_keys):\n",
    "            weights = model_state[layer_key]\n",
    "\n",
    "            # compute the weight of the neurons in this layer\n",
    "            if weights.dim() > 2:\n",
    "                l1_norms = torch.sum(torch.abs(weights), dim=(1, 2, 3))\n",
    "            else:\n",
    "                l1_norms = torch.sum(torch.abs(weights), dim=(1))\n",
    "\n",
    "            neuron_rankings = get_rankings(l1_norms)\n",
    "        \n",
    "            check_index = int(os.path.basename(checkpoint_path).split(\"_\")[1]) - 1 # get the index of the checkpoint\n",
    "            \n",
    "            redo_score = redo_scores[check_index][i]\n",
    "            redo_rankings = get_rankings(torch.tensor(redo_score))\n",
    "\n",
    "            k_corr, k_pval = scipy.stats.kendalltau(neuron_rankings, redo_rankings)\n",
    "            s_corr, s_pval = scipy.stats.spearmanr(neuron_rankings, redo_rankings)\n",
    "            p_corr, p_pval = scipy.stats.pearsonr(l1_norms, redo_score)\n",
    "\n",
    "            # also do an experiment with thresholding  \n",
    "\n",
    "            tau_vals = [0.025, 0.1]\n",
    "\n",
    "            for tau in tau_vals:\n",
    "                dormant_neurons_mask = (redo_score <= tau)\n",
    "                \n",
    "                dormant_neurons_idx = dormant_neurons_mask.nonzero().flatten()\n",
    "\n",
    "                only_redo, intersect, only_pruning = mask_vs_norm_stats(\n",
    "                    dormant_neurons_mask, l1_norms\n",
    "                )\n",
    "\n",
    "                new_row = {\"seed\": cfg[\"seed\"],\n",
    "                        \"env\": cfg[\"environment\"],\n",
    "                        \"experiment\": \"_\".join(cfg[\"experiment_name\"].split(\"_\")[:2]), # WARNING\n",
    "                        \"tau\": tau,\n",
    "                        \"nr_only_redo\": only_redo,\n",
    "                        \"nr_intersect\": intersect,\n",
    "                        \"nr_only_pruning\": only_pruning,\n",
    "                        \"dead_neuron_l1norms\": l1_norms[dormant_neurons_idx],\n",
    "                        \"dead_neuron_l1norms_avg\": np.average(l1_norms[dormant_neurons_idx]),\n",
    "                        \"dead_neuron_ratio\": len(dormant_neurons_idx) / len(redo_score),\n",
    "                        \"redo_l1norm_hamming\": mask_to_score_distance(dormant_neurons_mask, l1_norms),\n",
    "                        \"checkpoint\": os.path.basename(checkpoint_path),\n",
    "                        \"epoch\": check_index, \n",
    "                        \"layer\": layer_key,\n",
    "                        \"kendall_r_corr\": k_corr, \n",
    "                        \"kendall_r_pval\": k_pval,\n",
    "                        \"spearman_r_corr\": s_corr, \n",
    "                        \"spearman_r_pval\": s_pval,\n",
    "                        \"pearson_corr\": p_corr, \n",
    "                        \"pearson_pval\": p_pval\n",
    "                        }\n",
    "                results.append(new_row)\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    return df \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_analysis_results = []\n",
    "\n",
    "for exp_paths in experiment_paths:\n",
    "    exp_analysis_results.append(compare_redo_pruning(exp_paths))\n",
    "\n",
    "# concatenate the dataframes vertically\n",
    "df_res = pd.concat(exp_analysis_results)\n",
    "df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_col = 'dead_neuron_ratio'\n",
    "\n",
    "experiment_order = ['conv8_lin32', 'conv16_lin64', 'conv32_lin128', 'conv64_lin256']\n",
    "\n",
    "sub_df = df_res[df_res[\"tau\"] == 0.025]\n",
    "\n",
    "# Compute the average metric between the seeds for each experiment\n",
    "avg_df = sub_df.groupby(['env', 'experiment', 'epoch', \"layer\"]).mean().reset_index()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "\n",
    "sns.lineplot(avg_df, x=\"epoch\", y=metric_col, hue='experiment', hue_order = experiment_order, ax = ax)\n",
    "\n",
    "fig.suptitle(metric_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "experiment_order = ['conv8_lin32', 'conv16_lin64', 'conv32_lin128', 'conv64_lin256']\n",
    "\n",
    "for layer in df_res[\"layer\"].unique():\n",
    "    sub_df = df_res[(df_res[\"tau\"] == 0.025) & (df_res[\"layer\"] == layer)]\n",
    "\n",
    "    melted_df = sub_df.melt(id_vars=['env', 'experiment', 'epoch'],\n",
    "                            value_vars=['nr_only_redo', 'nr_intersect', 'nr_only_pruning'],\n",
    "                            var_name='Category', value_name='Value')\n",
    "\n",
    "    fig = px.bar(melted_df, x=\"epoch\", y=\"Value\", color=\"Category\", facet_row=\"env\", facet_col=\"experiment\",\n",
    "                category_orders={\"experiment\": experiment_order}, \n",
    "                height=600)\n",
    "    \n",
    "    title = f\"Redo and Norm overlap - Layer {layer} shared Y\"\n",
    "    file_path = f\"imgs/{title}.png\"\n",
    "\n",
    "    # Save the figure as a PNG image\n",
    "    pio.write_image(fig, file_path)\n",
    "\n",
    "    fig.update_layout(title=title)\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "    melted_df = sub_df.melt(id_vars=['env', 'experiment', 'epoch'],\n",
    "                            value_vars=['nr_only_redo', 'nr_intersect', 'nr_only_pruning'],\n",
    "                            var_name='Category', value_name='Value')\n",
    "\n",
    "    fig = px.bar(melted_df, x=\"epoch\", y=\"Value\", color=\"Category\", facet_row=\"env\", facet_col=\"experiment\",\n",
    "                category_orders={\"experiment\": experiment_order}, height=600)\n",
    "\n",
    "    fig.update_yaxes(matches=None, showticklabels=True)\n",
    "\n",
    "    title = f\"Redo and Norm overlap - Layer {layer}\"\n",
    "    file_path = f\"imgs/{title}.png\"\n",
    "    \n",
    "    # Save the figure as a PNG image\n",
    "    pio.write_image(fig, file_path)\n",
    "\n",
    "    fig.update_layout(title=title)\n",
    "\n",
    "    fig.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
