{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "proj_root = os.path.dirname(os.path.abspath(\".\"))\n",
    "# print(proj_root)\n",
    "sys.path.append(proj_root)\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import gym\n",
    "import yaml\n",
    "\n",
    "from minatar_dqn.my_dqn import AgentDQN\n",
    "from minatar_dqn.replay_buffer import ReplayBuffer\n",
    "from minatar_dqn.utils.my_logging import setup_logger\n",
    "from minatar_dqn.models import Conv_QNET, Conv_QNET_one\n",
    "\n",
    "from experiments.experiment_utils import (\n",
    "    seed_everything,\n",
    "    search_files_containing_string,\n",
    "    split_path_at_substring,\n",
    "    collect_training_output_files,\n",
    ")\n",
    "\n",
    "from minatar_dqn.my_dqn import Conv_QNET, build_environment\n",
    "from minatar_dqn.redo import (\n",
    "    apply_redo_parametrization,\n",
    "    reset_optimizer_states,\n",
    "    map_layers_to_optimizer_indices,\n",
    ")\n",
    "from experiments.experiment_utils import (\n",
    "    collect_training_output_files,\n",
    "    collect_pruning_output_files,\n",
    ")\n",
    "\n",
    "from experiments.training.training import read_config_files, get_config_paths\n",
    "\n",
    "from flatten_dict import flatten\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "\n",
    "sns.set()\n",
    "\n",
    "import plotly\n",
    "\n",
    "plotly.io.kaleido.scope.mathjax = None\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'epochs_to_train': 20,\n",
       " 'seeds': [0],\n",
       " 'environments': ['breakout'],\n",
       " 'agent_params': {'agent': 'AgentDQN',\n",
       "  'args_': {'train_step_cnt': 200000,\n",
       "   'validation_enabled': True,\n",
       "   'validation_step_cnt': 125000,\n",
       "   'validation_epsilon': 0.001,\n",
       "   'replay_start_size': 5000,\n",
       "   'batch_size': 32,\n",
       "   'training_freq': 4,\n",
       "   'target_model_update_freq': 100,\n",
       "   'loss_fcn': 'mse_loss',\n",
       "   'gamma': 0.99,\n",
       "   'epsilon': {'start': 1.0, 'end': 0.01, 'decay': 250000}}},\n",
       " 'estimator': {'model': 'Conv_QNET',\n",
       "  'args_': {'conv_hidden_out_size': 32, 'lin_hidden_out_size': 128}},\n",
       " 'optim': {'name': 'Adam', 'args_': {'lr': 6.25e-05, 'eps': 0.00015}},\n",
       " 'replay_buffer': {'max_size': 100000, 'action_dim': 1, 'n_step': 0},\n",
       " 'redo': {'attach': True,\n",
       "  'enabled': False,\n",
       "  'tau': 0.025,\n",
       "  'beta': 0.1,\n",
       "  'selection_option': None},\n",
       " 'reward_perception': None,\n",
       " 'experiment_name': 'conv32_lin128'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup config\n",
    "root_dir = os.path.dirname(os.path.abspath(\".\"))\n",
    "\n",
    "path_experiments_configs = os.path.join(\n",
    "    root_dir, \"experiments\", \"training\", \"training_configs\"\n",
    ")\n",
    "path_experiments_outputs = os.path.join(root_dir, \"experiments\", \"training\", \"outputs\")\n",
    "\n",
    "default_config_path, experiment_config_paths = get_config_paths(\n",
    "    path_experiments_configs\n",
    ")\n",
    "\n",
    "experiment_configs = read_config_files(default_config_path, experiment_config_paths)\n",
    "\n",
    "experiment_configs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = experiment_configs[0]\n",
    "config[\"environment\"] = \"breakout\"\n",
    "config[\"seed\"] = 0\n",
    "\n",
    "output_path = os.path.join(\n",
    "    root_dir,\n",
    "    \"experiments\",\n",
    "    \"redo\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'epochs_to_train': 20,\n",
       " 'seeds': [0],\n",
       " 'environments': ['breakout'],\n",
       " 'agent_params': {'agent': 'AgentDQN',\n",
       "  'args_': {'train_step_cnt': 200000,\n",
       "   'validation_enabled': True,\n",
       "   'validation_step_cnt': 125000,\n",
       "   'validation_epsilon': 0.001,\n",
       "   'replay_start_size': 5000,\n",
       "   'batch_size': 32,\n",
       "   'training_freq': 4,\n",
       "   'target_model_update_freq': 100,\n",
       "   'loss_fcn': 'mse_loss',\n",
       "   'gamma': 0.99,\n",
       "   'epsilon': {'start': 1.0, 'end': 0.01, 'decay': 250000}}},\n",
       " 'estimator': {'model': 'Conv_QNET',\n",
       "  'args_': {'conv_hidden_out_size': 32, 'lin_hidden_out_size': 128}},\n",
       " 'optim': {'name': 'Adam', 'args_': {'lr': 6.25e-05, 'eps': 0.00015}},\n",
       " 'replay_buffer': {'max_size': 100000, 'action_dim': 1, 'n_step': 0},\n",
       " 'redo': {'attach': True,\n",
       "  'enabled': False,\n",
       "  'tau': 0.025,\n",
       "  'beta': 0.1,\n",
       "  'selection_option': None},\n",
       " 'reward_perception': None,\n",
       " 'experiment_name': 'conv32_lin128',\n",
       " 'environment': 'breakout',\n",
       " 'seed': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"agent_params\"][\"args_\"][\"train_step_cnt\"] = 20000\n",
    "config[\"agent_params\"][\"args_\"][\"validation_step_cnt\"] = 12500\n",
    "config[\"redo\"][\"beta\"] = 1\n",
    "config[\"redo\"][\"tau\"] = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-01 23:53:54,236 - root - INFO - redo_test_experiment - Starting up experiment: conv32_lin128, environment: breakout, seed: 0\n",
      "2023-11-01 23:53:54,241 - root - INFO - redo_test_experiment - Loaded configuration settings.\n",
      "2023-11-01 23:53:54,248 - root - INFO - redo_test_experiment - Initialized newtworks and optimizer.\n",
      "2023-11-01 23:53:54,248 - root - INFO - redo_test_experiment - Applied redo parametrization to policy model.\n",
      "2023-11-01 23:53:54,249 - root - INFO - redo_test_experiment - Applied redo parametrization to target model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Chainsword\\anaconda3\\envs\\general\\lib\\site-packages\\gym\\envs\\registration.py:555: UserWarning:\n",
      "\n",
      "\u001b[33mWARN: The environment MinAtar/Breakout-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "\n",
      "c:\\Users\\Chainsword\\anaconda3\\envs\\general\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:20: UserWarning:\n",
      "\n",
      "\u001b[33mWARN: It seems a Box observation space is an image but the `dtype` is not `np.uint8`, actual type: bool. If the Box observation space is not an image, we recommend flattening the observation to have only a 1D vector.\u001b[0m\n",
      "\n",
      "c:\\Users\\Chainsword\\anaconda3\\envs\\general\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:25: UserWarning:\n",
      "\n",
      "\u001b[33mWARN: It seems a Box observation space is an image but the upper and lower bounds are not in [0, 255]. Generally, CNN policies assume observations are within that range, so you may encounter an issue if the observation values are not.\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env_name = config[\"environment\"]\n",
    "\n",
    "logger = setup_logger(\n",
    "    env_name=env_name,\n",
    "    identifier_string=\"redo_test_experiment\",\n",
    ")\n",
    "logger.info(\n",
    "    f'Starting up experiment: {config[\"experiment_name\"]}, environment: {config[\"environment\"]}, seed: {config[\"seed\"]}'\n",
    ")\n",
    "\n",
    "### Setup environments ###\n",
    "train_env = build_environment(\n",
    "    game_name=config[\"environment\"], random_seed=config[\"seed\"]\n",
    ")\n",
    "validation_env = build_environment(\n",
    "    game_name=config[\"environment\"], random_seed=config[\"seed\"]\n",
    ")\n",
    "\n",
    "experiment_agent = AgentDQN(\n",
    "    train_env=train_env,\n",
    "    validation_env=validation_env,\n",
    "    experiment_output_folder=output_path,\n",
    "    experiment_name=\"redo_test_experiment\",\n",
    "    resume_training_path=None,\n",
    "    save_checkpoints=True,\n",
    "    logger=logger,\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-01 23:53:54,308 - root - INFO - redo_test_experiment - Starting training session at: 0\n",
      "2023-11-01 23:53:54,308 - root - INFO - redo_test_experiment - Starting training epoch at t = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Chainsword\\anaconda3\\envs\\general\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning:\n",
      "\n",
      "`np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-01 23:54:19,208 - root - INFO - redo_test_experiment - TRAINING STATS | Frames seen: 20000 | Episode: 1750 | Max reward: 5.0 | Avg reward: 0.532 | Avg frames (episode): 11.425142857142857 | Avg max Q: -69.70157273895877 | Epsilon: 0.9406 | Train epoch time: 0:00:24.895504\n",
      "2023-11-01 23:54:19,209 - root - INFO - redo_test_experiment - Starting validation epoch at t = 20000\n",
      "2023-11-01 23:54:24,917 - root - INFO - redo_test_experiment - VALIDATION STATS | Max reward: 1.0 | Avg reward: 0.003864734299516908 | Avg frames (episode): 6.038647342995169 | Avg max Q: -87.59372736653678 | Validation epoch time: 0:00:05.706717\n",
      "2023-11-01 23:54:24,918 - root - INFO - redo_test_experiment - Saving checkpoint at t = 20000 ...\n",
      "2023-11-01 23:54:24,922 - root - DEBUG - redo_test_experiment - Models saved at t = 20000\n",
      "2023-11-01 23:54:24,927 - root - DEBUG - redo_test_experiment - Training status saved at t = 20000\n",
      "2023-11-01 23:54:26,210 - root - INFO - redo_test_experiment - Checkpoint saved at t = 20000\n",
      "2023-11-01 23:54:26,211 - root - INFO - redo_test_experiment - Epoch 0 completed in 0:00:31.902649\n",
      "2023-11-01 23:54:26,211 - root - INFO - redo_test_experiment - \n",
      "\n",
      "2023-11-01 23:54:26,211 - root - INFO - redo_test_experiment - Starting training epoch at t = 20000\n",
      "2023-11-01 23:55:00,826 - root - INFO - redo_test_experiment - TRAINING STATS | Frames seen: 40000 | Episode: 3568 | Max reward: 8.0 | Avg reward: 0.49064906490649063 | Avg frames (episode): 11.004400440044005 | Avg max Q: -89.24336225782683 | Epsilon: 0.8613999999999999 | Train epoch time: 0:00:34.609507\n",
      "2023-11-01 23:55:00,826 - root - INFO - redo_test_experiment - Starting validation epoch at t = 40000\n",
      "2023-11-01 23:55:07,045 - root - INFO - redo_test_experiment - VALIDATION STATS | Max reward: 1.0 | Avg reward: 0.0009615384615384616 | Avg frames (episode): 6.009615384615385 | Avg max Q: -90.74193951625318 | Validation epoch time: 0:00:06.216008\n",
      "2023-11-01 23:55:07,046 - root - INFO - redo_test_experiment - Saving checkpoint at t = 40000 ...\n",
      "2023-11-01 23:55:07,049 - root - DEBUG - redo_test_experiment - Models saved at t = 40000\n",
      "2023-11-01 23:55:07,058 - root - DEBUG - redo_test_experiment - Training status saved at t = 40000\n",
      "2023-11-01 23:55:09,997 - root - INFO - redo_test_experiment - Checkpoint saved at t = 40000\n",
      "2023-11-01 23:55:09,998 - root - INFO - redo_test_experiment - Epoch 1 completed in 0:00:43.786940\n",
      "2023-11-01 23:55:09,998 - root - INFO - redo_test_experiment - \n",
      "\n",
      "2023-11-01 23:55:09,999 - root - INFO - redo_test_experiment - Starting training epoch at t = 40000\n",
      "2023-11-01 23:55:48,541 - root - INFO - redo_test_experiment - TRAINING STATS | Frames seen: 60000 | Episode: 5307 | Max reward: 6.0 | Avg reward: 0.5370902817711328 | Avg frames (episode): 11.492811960897066 | Avg max Q: -87.86776931924932 | Epsilon: 0.7822 | Train epoch time: 0:00:38.538197\n",
      "2023-11-01 23:55:48,542 - root - INFO - redo_test_experiment - Starting validation epoch at t = 60000\n",
      "2023-11-01 23:55:54,839 - root - INFO - redo_test_experiment - VALIDATION STATS | Max reward: 2.0 | Avg reward: 1.3083969465648855 | Avg frames (episode): 19.083969465648856 | Avg max Q: -85.99086813623664 | Validation epoch time: 0:00:06.294517\n",
      "2023-11-01 23:55:54,839 - root - INFO - redo_test_experiment - Saving checkpoint at t = 60000 ...\n",
      "2023-11-01 23:55:54,843 - root - DEBUG - redo_test_experiment - Models saved at t = 60000\n",
      "2023-11-01 23:55:54,861 - root - DEBUG - redo_test_experiment - Training status saved at t = 60000\n",
      "2023-11-01 23:55:59,213 - root - INFO - redo_test_experiment - Checkpoint saved at t = 60000\n",
      "2023-11-01 23:55:59,214 - root - INFO - redo_test_experiment - Epoch 2 completed in 0:00:49.215572\n",
      "2023-11-01 23:55:59,214 - root - INFO - redo_test_experiment - \n",
      "\n",
      "2023-11-01 23:55:59,215 - root - INFO - redo_test_experiment - Starting training epoch at t = 60000\n",
      "2023-11-01 23:56:38,437 - root - INFO - redo_test_experiment - TRAINING STATS | Frames seen: 80000 | Episode: 6967 | Max reward: 6.0 | Avg reward: 0.591566265060241 | Avg frames (episode): 12.048192771084338 | Avg max Q: -84.6784490177007 | Epsilon: 0.7030000000000001 | Train epoch time: 0:00:39.217168\n",
      "2023-11-01 23:56:38,438 - root - INFO - redo_test_experiment - Starting validation epoch at t = 80000\n",
      "2023-11-01 23:56:44,336 - root - INFO - redo_test_experiment - VALIDATION STATS | Max reward: 3.0 | Avg reward: 1.3317901234567902 | Avg frames (episode): 19.32716049382716 | Avg max Q: -82.82927742345973 | Validation epoch time: 0:00:05.896124\n",
      "2023-11-01 23:56:44,336 - root - INFO - redo_test_experiment - Saving checkpoint at t = 80000 ...\n",
      "2023-11-01 23:56:44,340 - root - DEBUG - redo_test_experiment - Models saved at t = 80000\n",
      "2023-11-01 23:56:44,360 - root - DEBUG - redo_test_experiment - Training status saved at t = 80000\n",
      "2023-11-01 23:56:49,964 - root - INFO - redo_test_experiment - Checkpoint saved at t = 80000\n",
      "2023-11-01 23:56:49,965 - root - INFO - redo_test_experiment - Epoch 3 completed in 0:00:50.749857\n",
      "2023-11-01 23:56:49,965 - root - INFO - redo_test_experiment - \n",
      "\n",
      "2023-11-01 23:56:49,966 - root - INFO - redo_test_experiment - Starting training epoch at t = 80000\n",
      "2023-11-01 23:57:29,951 - root - INFO - redo_test_experiment - TRAINING STATS | Frames seen: 100000 | Episode: 8564 | Max reward: 7.0 | Avg reward: 0.638697557921102 | Avg frames (episode): 12.524733876017534 | Avg max Q: -82.55190418856395 | Epsilon: 0.6238 | Train epoch time: 0:00:39.980174\n",
      "2023-11-01 23:57:29,951 - root - INFO - redo_test_experiment - Starting validation epoch at t = 100000\n",
      "2023-11-01 23:57:35,741 - root - INFO - redo_test_experiment - VALIDATION STATS | Max reward: 2.0 | Avg reward: 1.392 | Avg frames (episode): 20.0096 | Avg max Q: -80.34134472440884 | Validation epoch time: 0:00:05.787224\n",
      "2023-11-01 23:57:35,742 - root - INFO - redo_test_experiment - Saving checkpoint at t = 100000 ...\n",
      "2023-11-01 23:57:35,746 - root - DEBUG - redo_test_experiment - Models saved at t = 100000\n",
      "2023-11-01 23:57:35,768 - root - DEBUG - redo_test_experiment - Training status saved at t = 100000\n",
      "2023-11-01 23:57:42,402 - root - INFO - redo_test_experiment - Checkpoint saved at t = 100000\n",
      "2023-11-01 23:57:42,402 - root - INFO - redo_test_experiment - Epoch 4 completed in 0:00:52.435894\n",
      "2023-11-01 23:57:42,402 - root - INFO - redo_test_experiment - \n",
      "\n",
      "2023-11-01 23:57:42,403 - root - INFO - redo_test_experiment - Starting training epoch at t = 100000\n",
      "2023-11-01 23:58:22,066 - root - INFO - redo_test_experiment - TRAINING STATS | Frames seen: 120000 | Episode: 9974 | Max reward: 6.0 | Avg reward: 0.7957446808510639 | Avg frames (episode): 14.180141843971631 | Avg max Q: -82.79933702288379 | Epsilon: 0.5446 | Train epoch time: 0:00:39.658379\n",
      "2023-11-01 23:58:22,067 - root - INFO - redo_test_experiment - Starting validation epoch at t = 120000\n",
      "2023-11-01 23:58:27,653 - root - INFO - redo_test_experiment - VALIDATION STATS | Max reward: 6.0 | Avg reward: 2.677685950413223 | Avg frames (episode): 34.46280991735537 | Avg max Q: -81.98908034057617 | Validation epoch time: 0:00:05.584305\n",
      "2023-11-01 23:58:27,653 - root - INFO - redo_test_experiment - Saving checkpoint at t = 120000 ...\n",
      "2023-11-01 23:58:27,658 - root - DEBUG - redo_test_experiment - Models saved at t = 120000\n",
      "2023-11-01 23:58:27,682 - root - DEBUG - redo_test_experiment - Training status saved at t = 120000\n",
      "2023-11-01 23:58:34,517 - root - INFO - redo_test_experiment - Checkpoint saved at t = 120000\n",
      "2023-11-01 23:58:34,518 - root - INFO - redo_test_experiment - Epoch 5 completed in 0:00:52.114616\n",
      "2023-11-01 23:58:34,519 - root - INFO - redo_test_experiment - \n",
      "\n",
      "2023-11-01 23:58:34,519 - root - INFO - redo_test_experiment - Starting training epoch at t = 120000\n",
      "2023-11-01 23:59:15,316 - root - INFO - redo_test_experiment - TRAINING STATS | Frames seen: 140000 | Episode: 11111 | Max reward: 7.0 | Avg reward: 1.1187335092348285 | Avg frames (episode): 17.60246262093228 | Avg max Q: -81.24642752885552 | Epsilon: 0.4653999999999999 | Train epoch time: 0:00:40.791867\n",
      "2023-11-01 23:59:15,316 - root - INFO - redo_test_experiment - Starting validation epoch at t = 140000\n",
      "2023-11-01 23:59:21,061 - root - INFO - redo_test_experiment - VALIDATION STATS | Max reward: 6.0 | Avg reward: 2.7355371900826446 | Avg frames (episode): 34.47933884297521 | Avg max Q: -80.0852304341935 | Validation epoch time: 0:00:05.743195\n",
      "2023-11-01 23:59:21,062 - root - INFO - redo_test_experiment - Saving checkpoint at t = 140000 ...\n",
      "2023-11-01 23:59:21,065 - root - DEBUG - redo_test_experiment - Models saved at t = 140000\n",
      "2023-11-01 23:59:21,096 - root - DEBUG - redo_test_experiment - Training status saved at t = 140000\n",
      "2023-11-01 23:59:28,053 - root - INFO - redo_test_experiment - Checkpoint saved at t = 140000\n",
      "2023-11-01 23:59:28,054 - root - INFO - redo_test_experiment - Epoch 6 completed in 0:00:53.535190\n",
      "2023-11-01 23:59:28,054 - root - INFO - redo_test_experiment - \n",
      "\n",
      "2023-11-01 23:59:28,055 - root - INFO - redo_test_experiment - Starting training epoch at t = 140000\n",
      "2023-11-02 00:00:10,180 - root - INFO - redo_test_experiment - TRAINING STATS | Frames seen: 160000 | Episode: 12006 | Max reward: 7.0 | Avg reward: 1.559776536312849 | Avg frames (episode): 22.32849162011173 | Avg max Q: -78.16781395993918 | Epsilon: 0.3862 | Train epoch time: 0:00:42.119828\n",
      "2023-11-02 00:00:10,180 - root - INFO - redo_test_experiment - Starting validation epoch at t = 160000\n",
      "2023-11-02 00:00:15,978 - root - INFO - redo_test_experiment - VALIDATION STATS | Max reward: 3.0 | Avg reward: 2.514360313315927 | Avg frames (episode): 32.652741514360315 | Avg max Q: -75.58118729611608 | Validation epoch time: 0:00:05.794141\n",
      "2023-11-02 00:00:15,978 - root - INFO - redo_test_experiment - Saving checkpoint at t = 160000 ...\n",
      "2023-11-02 00:00:15,982 - root - DEBUG - redo_test_experiment - Models saved at t = 160000\n",
      "2023-11-02 00:00:16,015 - root - DEBUG - redo_test_experiment - Training status saved at t = 160000\n",
      "2023-11-02 00:00:22,936 - root - INFO - redo_test_experiment - Checkpoint saved at t = 160000\n",
      "2023-11-02 00:00:22,937 - root - INFO - redo_test_experiment - Epoch 7 completed in 0:00:54.882374\n",
      "2023-11-02 00:00:22,937 - root - INFO - redo_test_experiment - \n",
      "\n",
      "2023-11-02 00:00:22,938 - root - INFO - redo_test_experiment - Starting training epoch at t = 160000\n",
      "2023-11-02 00:01:05,328 - root - INFO - redo_test_experiment - TRAINING STATS | Frames seen: 180000 | Episode: 12793 | Max reward: 7.0 | Avg reward: 1.846251588310038 | Avg frames (episode): 25.433290978398983 | Avg max Q: -73.10709499576036 | Epsilon: 0.30700000000000005 | Train epoch time: 0:00:42.384045\n",
      "2023-11-02 00:01:05,328 - root - INFO - redo_test_experiment - Starting validation epoch at t = 180000\n",
      "2023-11-02 00:01:11,137 - root - INFO - redo_test_experiment - VALIDATION STATS | Max reward: 6.0 | Avg reward: 2.9939577039274923 | Avg frames (episode): 37.86706948640484 | Avg max Q: -71.3836279724482 | Validation epoch time: 0:00:05.805666\n",
      "2023-11-02 00:01:11,137 - root - INFO - redo_test_experiment - Saving checkpoint at t = 180000 ...\n",
      "2023-11-02 00:01:11,141 - root - DEBUG - redo_test_experiment - Models saved at t = 180000\n",
      "2023-11-02 00:01:11,179 - root - DEBUG - redo_test_experiment - Training status saved at t = 180000\n",
      "2023-11-02 00:01:17,692 - root - INFO - redo_test_experiment - Checkpoint saved at t = 180000\n",
      "2023-11-02 00:01:17,693 - root - INFO - redo_test_experiment - Epoch 8 completed in 0:00:54.755105\n",
      "2023-11-02 00:01:17,693 - root - INFO - redo_test_experiment - \n",
      "\n",
      "2023-11-02 00:01:17,693 - root - INFO - redo_test_experiment - Starting training epoch at t = 180000\n",
      "2023-11-02 00:02:06,396 - root - INFO - redo_test_experiment - TRAINING STATS | Frames seen: 200000 | Episode: 13505 | Max reward: 7.0 | Avg reward: 2.0941011235955056 | Avg frames (episode): 28.09550561797753 | Avg max Q: -68.84914035130615 | Epsilon: 0.2278 | Train epoch time: 0:00:48.696659\n",
      "2023-11-02 00:02:06,397 - root - INFO - redo_test_experiment - Starting validation epoch at t = 200000\n",
      "2023-11-02 00:02:12,872 - root - INFO - redo_test_experiment - VALIDATION STATS | Max reward: 6.0 | Avg reward: 3.0337423312883436 | Avg frames (episode): 38.3680981595092 | Avg max Q: -67.24929624896583 | Validation epoch time: 0:00:06.473281\n",
      "2023-11-02 00:02:12,873 - root - INFO - redo_test_experiment - Saving checkpoint at t = 200000 ...\n",
      "2023-11-02 00:02:12,878 - root - DEBUG - redo_test_experiment - Models saved at t = 200000\n",
      "2023-11-02 00:02:12,930 - root - DEBUG - redo_test_experiment - Training status saved at t = 200000\n",
      "2023-11-02 00:02:21,319 - root - INFO - redo_test_experiment - Checkpoint saved at t = 200000\n",
      "2023-11-02 00:02:21,320 - root - INFO - redo_test_experiment - Epoch 9 completed in 0:01:03.627002\n",
      "2023-11-02 00:02:21,320 - root - INFO - redo_test_experiment - \n",
      "\n",
      "2023-11-02 00:02:21,321 - root - INFO - redo_test_experiment - Ended training session after 10 epochs at t = 200000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_agent.train(train_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect a set of states to check Q on before and after redo\n",
    "\n",
    "eval_states = []\n",
    "\n",
    "samples_nr = 100\n",
    "skip_nr = 100\n",
    "for i in range(samples_nr):\n",
    "    for j in range(skip_nr):\n",
    "        # get a random action from the environment\n",
    "        action = experiment_agent.train_env.action_space.sample()\n",
    "        s_prime, reward, is_terminated, truncated, info = experiment_agent.train_env.step(\n",
    "            action\n",
    "        )\n",
    "        s_prime = torch.tensor(s_prime, device=\"cpu\").float()\n",
    "\n",
    "        if is_terminated:\n",
    "            experiment_agent.train_env.reset()\n",
    "            continue\n",
    "\n",
    "    eval_states.append(s_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "eval_states_tensor = torch.stack(eval_states)\n",
    "\n",
    "predictions_init = experiment_agent.policy_model(eval_states_tensor)\n",
    "predictions_init\n",
    "\n",
    "max_q_vals_init = []\n",
    "for state in eval_states_tensor:\n",
    "    max_q_val = experiment_agent.get_max_q_val_for_state(state.unsqueeze(0))\n",
    "    max_q_vals_init.append(max_q_val)\n",
    "\n",
    "# get the weights before redo\n",
    "state_dict_init = copy.deepcopy(experiment_agent.policy_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'indexes': tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "          18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]),\n",
       "  'inbound': 'features.conv1',\n",
       "  'outbound': 'features.conv2'},\n",
       " {'indexes': tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "          18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]),\n",
       "  'inbound': 'features.conv2',\n",
       "  'outbound': 'fc.lin1'},\n",
       " {'indexes': tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "           14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
       "           28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
       "           42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
       "           56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
       "           70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
       "           84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
       "           98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
       "          112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
       "          126, 127]),\n",
       "  'inbound': 'fc.lin1',\n",
       "  'outbound': 'fc.lin2'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply redo\n",
    "reset_details = experiment_agent.policy_model.apply_redo()\n",
    "reset_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_to_optim_idx = map_layers_to_optimizer_indices(\n",
    "    experiment_agent.policy_model, experiment_agent.optimizer\n",
    ")\n",
    "layer_to_optim_idx\n",
    "\n",
    "reset_optimizer_states(reset_details, experiment_agent.optimizer, layer_to_optim_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the difference between the values before and after\n",
    "predictions_after = experiment_agent.policy_model(eval_states_tensor)\n",
    "predictions_after\n",
    "\n",
    "max_q_vals_after = []\n",
    "for state in eval_states_tensor:\n",
    "    max_q_val = experiment_agent.get_max_q_val_for_state(state.unsqueeze(0))\n",
    "    max_q_vals_after.append(max_q_val)\n",
    "\n",
    "# get the weights before redo\n",
    "state_dict_after = experiment_agent.policy_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6218.0907286554575"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff = np.array(max_q_vals_init) - np.array(max_q_vals_after)\n",
    "sum(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-64.28768157958984,\n",
       " -64.709228515625,\n",
       " -68.58647918701172,\n",
       " -67.16693878173828,\n",
       " -83.45682525634766,\n",
       " -63.717437744140625,\n",
       " -63.442543029785156,\n",
       " -66.2746353149414,\n",
       " -61.1007080078125,\n",
       " -58.52145004272461,\n",
       " -58.478302001953125,\n",
       " -59.55796813964844,\n",
       " -60.09797286987305,\n",
       " -61.80244064331055,\n",
       " -62.32272720336914,\n",
       " -63.70777130126953,\n",
       " -64.34134674072266,\n",
       " -65.33708953857422,\n",
       " -64.15252685546875,\n",
       " -72.15023040771484,\n",
       " -63.717437744140625,\n",
       " -63.442543029785156,\n",
       " -62.82538604736328,\n",
       " -61.1007080078125,\n",
       " -58.52145004272461,\n",
       " -58.478302001953125,\n",
       " -59.55796813964844,\n",
       " -60.09797286987305,\n",
       " -61.80244064331055,\n",
       " -62.32272720336914,\n",
       " -66.83450317382812,\n",
       " -64.34134674072266,\n",
       " -66.03160095214844,\n",
       " -76.37629699707031,\n",
       " -75.23518371582031,\n",
       " -64.28768157958984,\n",
       " -68.33306884765625,\n",
       " -68.58647918701172,\n",
       " -67.16693878173828,\n",
       " -83.45682525634766,\n",
       " -63.662872314453125,\n",
       " -64.76863861083984,\n",
       " -65.44690704345703,\n",
       " -63.429786682128906,\n",
       " -59.09043884277344,\n",
       " -58.70704650878906,\n",
       " -59.836082458496094,\n",
       " -60.73176956176758,\n",
       " -63.641639709472656,\n",
       " -64.71994018554688,\n",
       " -67.48727416992188,\n",
       " -68.09233093261719,\n",
       " -67.41928100585938,\n",
       " -66.07769775390625,\n",
       " -63.498443603515625,\n",
       " -62.368831634521484,\n",
       " -63.64763259887695,\n",
       " -67.68242645263672,\n",
       " -71.05471801757812,\n",
       " -69.63289642333984,\n",
       " -70.56211853027344,\n",
       " -69.19483184814453,\n",
       " -68.08418273925781,\n",
       " -71.16474151611328,\n",
       " -76.35138702392578,\n",
       " -81.47557830810547,\n",
       " -91.5125503540039,\n",
       " -63.15523147583008,\n",
       " -63.9538688659668,\n",
       " -64.8205337524414,\n",
       " -66.63719940185547,\n",
       " -59.09043884277344,\n",
       " -58.70704650878906,\n",
       " -59.836082458496094,\n",
       " -61.24650573730469,\n",
       " -63.641639709472656,\n",
       " -67.38668060302734,\n",
       " -70.166748046875,\n",
       " -72.66796112060547,\n",
       " -75.7994384765625,\n",
       " -78.6699447631836,\n",
       " -86.81806945800781,\n",
       " -63.717437744140625,\n",
       " -63.442543029785156,\n",
       " -62.82538604736328,\n",
       " -61.1007080078125,\n",
       " -58.52145004272461,\n",
       " -58.478302001953125,\n",
       " -60.79658508300781,\n",
       " -69.51596069335938,\n",
       " -68.5270767211914,\n",
       " -69.04736328125,\n",
       " -70.43241119384766,\n",
       " -67.46807098388672]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_q_vals_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131,\n",
       " 0.06966846436262131]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_q_vals_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights changed in layer: features.conv1.weight\n",
      "Weights changed in layer: features.conv1.bias\n",
      "Weights changed in layer: features.relu1.running_avg\n",
      "Weights changed in layer: features.relu1.running_avg_cnt\n",
      "Weights changed in layer: features.conv2.weight\n",
      "Weights changed in layer: features.conv2.bias\n",
      "Weights changed in layer: features.relu2.running_avg\n",
      "Weights changed in layer: features.relu2.running_avg_cnt\n",
      "Weights changed in layer: fc.lin1.weight\n",
      "Weights changed in layer: fc.lin1.bias\n",
      "Weights changed in layer: fc.relu3.running_avg\n",
      "Weights changed in layer: fc.relu3.running_avg_cnt\n",
      "Weights changed in layer: fc.lin2.weight\n"
     ]
    }
   ],
   "source": [
    "for layer in state_dict_after:\n",
    "    if not torch.equal(state_dict_init[layer], state_dict_after[layer]):\n",
    "        print(f\"Weights changed in layer: {layer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
