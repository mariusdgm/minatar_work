{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marius\\anaconda3\\envs\\general\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import datetime\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "from collections import deque, Counter, namedtuple\n",
    "\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "\n",
    "from minatar import Environment\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from replay_buffer import ReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_dqn import AgentDQN, setup_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = \"breakout\"\n",
    "proj_dir = os.path.abspath(\".\")\n",
    "default_checkpoint_folder = os.path.join(proj_dir, \"checkpoints\", game)\n",
    "\n",
    "checkpoint_folder = default_checkpoint_folder\n",
    "\n",
    "model_file_name = os.path.join(checkpoint_folder, game + \"_model\")\n",
    "replay_buffer_file = os.path.join(checkpoint_folder, game + \"_replay_buffer\")\n",
    "train_stats_file = os.path.join(checkpoint_folder, game + \"_train_stats\")\n",
    "logs_path = os.path.join(checkpoint_folder, \"logs\")\n",
    "\n",
    "Path(checkpoint_folder).mkdir(parents=True, exist_ok=True)\n",
    "Path(logs_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "env = Environment(game)\n",
    "\n",
    "train_logger = setup_logger(game, logs_path)\n",
    "\n",
    "# print(\"Cuda available?: \" + str(torch.cuda.is_available()))\n",
    "my_agent = AgentDQN(\n",
    "        env=env,\n",
    "        model_file=model_file_name,\n",
    "        replay_buffer_file=replay_buffer_file,\n",
    "        train_stats_file=train_stats_file,\n",
    "        save_checkpoints=True,\n",
    "        logger=train_logger,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-30 01:41:04,959 - root - INFO - Starting/resuming training session at: 0\n",
      "2023-01-30 01:41:04,960 - root - INFO - Starting training epoch at t = 0\n",
      "2023-01-30 01:41:05,906 - root - INFO - Frames seen: 2000 | Episode: 173 | Max reward: 3.0 | Avg reward: 0.55 | Avg frames (episode): 11.560693641618498 | Avg max Q: 0.09245185119410355 | Epsilon: 1.0 | Train epoch time: 0:00:00.943135\n",
      "2023-01-30 01:41:05,907 - root - INFO - Starting validation epoch at t = 2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 3.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 2.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 2.0, 1.0, 3.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 2.0, 0.0, 0.0, 0.0, 2.0, 1.0, 1.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 2.0, 2.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 3.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0]\n",
      "[6, 16, 6, 16, 6, 6, 16, 6, 6, 6, 16, 6, 38, 6, 16, 6, 6, 16, 6, 6, 16, 16, 16, 16, 6, 6, 16, 16, 6, 6, 6, 28, 26, 6, 6, 6, 6, 16, 6, 6, 26, 16, 6, 16, 6, 16, 6, 26, 6, 6, 16, 6, 6, 6, 16, 26, 6, 6, 6, 16, 6, 16, 16, 6, 6, 28, 6, 6, 6, 6, 16, 6, 6, 6, 16, 6, 6, 6, 6, 6, 6, 6, 16, 16, 6, 6, 6, 16, 16, 16, 6, 6, 6, 28, 16, 38, 6, 6, 6, 16, 6, 6, 6, 6, 16, 16, 16, 6, 16, 16, 6, 16, 6, 6, 16, 16, 6, 6, 16, 6, 16, 16, 6, 6, 6, 16, 28, 6, 6, 6, 26, 16, 16, 6, 28, 16, 6, 16, 6, 6, 6, 16, 6, 6, 16, 26, 26, 6, 6, 16, 6, 16, 16, 6, 16, 16, 16, 6, 6, 16, 6, 16, 16, 16, 38, 16, 6, 6, 16, 6, 6, 16, 12]\n",
      "[]\n",
      "[0.09447666257619858, 0.09716878831386566, 0.08416585624217987, 0.1005772054195404, 0.09164784848690033, 0.07883855700492859, 0.09135293960571289, 0.08477672934532166, 0.09790296852588654, 0.0901593416929245, 0.09307889640331268, 0.10527642071247101]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-30 01:41:06,724 - root - INFO - Max reward: 1.0 | Avg reward: 0.49 | Avg frames (episode): 10.869565217391305 | Avg max Q: 0.09257919527590275 | Validation epoch time: 0:00:00.813331\n",
      "2023-01-30 01:41:06,725 - root - INFO - Saving checkpoint at t = 2000 ...\n",
      "c:\\Users\\Marius\\anaconda3\\envs\\general\\lib\\site-packages\\h5py\\_hl\\base.py:118: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  data = np.asarray(data, order=\"C\", dtype=as_dtype)\n",
      "c:\\Users\\Marius\\anaconda3\\envs\\general\\lib\\site-packages\\h5py\\_hl\\base.py:118: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  data = np.asarray(data, order=\"C\", dtype=as_dtype)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Object dtype dtype('O') has no native HDF5 equivalent",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m my_agent\u001b[39m.\u001b[39;49mtrain(\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[1;32md:\\Facultate\\PhD\\work\\learning_stuff\\dqn_clean\\minatar_work\\my_dqn.py:302\u001b[0m, in \u001b[0;36mAgentDQN.train\u001b[1;34m(self, train_epochs)\u001b[0m\n\u001b[0;32m    299\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_stats_to_log(validation_stats, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalidation_stats)\n\u001b[0;32m    301\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_checkpoints:\n\u001b[1;32m--> 302\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msave_checkpoint(\n\u001b[0;32m    303\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_file, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreplay_buffer_file, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_stats_file\n\u001b[0;32m    304\u001b[0m         )\n\u001b[0;32m    306\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEnded training session after \u001b[39m\u001b[39m{\u001b[39;00mtrain_epochs\u001b[39m}\u001b[39;00m\u001b[39m at t = \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mt\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Facultate\\PhD\\work\\learning_stuff\\dqn_clean\\minatar_work\\my_dqn.py:227\u001b[0m, in \u001b[0;36mAgentDQN.save_checkpoint\u001b[1;34m(self, model_file, replay_buffer_file, training_stats_file)\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_model(model_file)\n\u001b[0;32m    226\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_training_status(training_stats_file)\n\u001b[1;32m--> 227\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreplay_buffer\u001b[39m.\u001b[39;49msave(replay_buffer_file)\n\u001b[0;32m    228\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCheckpoint saved at t = \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mt\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Facultate\\PhD\\work\\learning_stuff\\dqn_clean\\minatar_work\\replay_buffer.py:73\u001b[0m, in \u001b[0;36mReplayBuffer.save\u001b[1;34m(self, file_name)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msave\u001b[39m(\u001b[39mself\u001b[39m, file_name):\n\u001b[0;32m     72\u001b[0m     \u001b[39mwith\u001b[39;00m h5py\u001b[39m.\u001b[39mFile(file_name, \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m---> 73\u001b[0m         states \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39;49mcreate_dataset(\n\u001b[0;32m     74\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mstates\u001b[39;49m\u001b[39m\"\u001b[39;49m, data\u001b[39m=\u001b[39;49m[s \u001b[39mfor\u001b[39;49;00m s, a, r, ns, d \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuffer]\n\u001b[0;32m     75\u001b[0m         )\n\u001b[0;32m     76\u001b[0m         actions \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mcreate_dataset(\n\u001b[0;32m     77\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mactions\u001b[39m\u001b[39m\"\u001b[39m, data\u001b[39m=\u001b[39m[a \u001b[39mfor\u001b[39;00m s, a, r, ns, d \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer]\n\u001b[0;32m     78\u001b[0m         )\n\u001b[0;32m     79\u001b[0m         rewards \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mcreate_dataset(\n\u001b[0;32m     80\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mrewards\u001b[39m\u001b[39m\"\u001b[39m, data\u001b[39m=\u001b[39m[r \u001b[39mfor\u001b[39;00m s, a, r, ns, d \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer]\n\u001b[0;32m     81\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\Marius\\anaconda3\\envs\\general\\lib\\site-packages\\h5py\\_hl\\group.py:161\u001b[0m, in \u001b[0;36mGroup.create_dataset\u001b[1;34m(self, name, shape, dtype, data, **kwds)\u001b[0m\n\u001b[0;32m    158\u001b[0m         parent_path, name \u001b[39m=\u001b[39m name\u001b[39m.\u001b[39mrsplit(\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m    159\u001b[0m         group \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequire_group(parent_path)\n\u001b[1;32m--> 161\u001b[0m dsid \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mmake_new_dset(group, shape, dtype, data, name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    162\u001b[0m dset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mDataset(dsid)\n\u001b[0;32m    163\u001b[0m \u001b[39mreturn\u001b[39;00m dset\n",
      "File \u001b[1;32mc:\\Users\\Marius\\anaconda3\\envs\\general\\lib\\site-packages\\h5py\\_hl\\dataset.py:88\u001b[0m, in \u001b[0;36mmake_new_dset\u001b[1;34m(parent, shape, dtype, data, name, chunks, compression, shuffle, fletcher32, maxshape, compression_opts, fillvalue, scaleoffset, track_times, external, track_order, dcpl, dapl, efile_prefix, virtual_prefix, allow_unknown_filter)\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     87\u001b[0m         dtype \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39mdtype(dtype)\n\u001b[1;32m---> 88\u001b[0m     tid \u001b[39m=\u001b[39m h5t\u001b[39m.\u001b[39;49mpy_create(dtype, logical\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m     90\u001b[0m \u001b[39m# Legacy\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39m((compression, shuffle, fletcher32, maxshape, scaleoffset)) \u001b[39mand\u001b[39;00m chunks \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n",
      "File \u001b[1;32mh5py\\h5t.pyx:1663\u001b[0m, in \u001b[0;36mh5py.h5t.py_create\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\h5t.pyx:1687\u001b[0m, in \u001b[0;36mh5py.h5t.py_create\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\h5t.pyx:1747\u001b[0m, in \u001b[0;36mh5py.h5t.py_create\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Object dtype dtype('O') has no native HDF5 equivalent"
     ]
    }
   ],
   "source": [
    "my_agent.train(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_agent.replay_buffer.save(replay_buffer_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 10, 10])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = get_state(my_agent.env.state())\n",
    "s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.data.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = my_agent.select_action(s, my_agent.t, my_agent.num_actions)\n",
    "reward, is_terminated = my_agent.env.act(action)\n",
    "reward = torch.tensor([[reward]], device=\"cpu\").float()\n",
    "is_terminated = torch.tensor([[is_terminated]], device=\"cpu\")\n",
    "s_prime = get_state(my_agent.env.state())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_prime.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = my_agent.replay_buffer.sample(1)\n",
    "len(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[0][0][0][0][0]\n",
    "type(sample[0][0][0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "state, action, reward, next_state, terminated = sample\n",
    "state = torch.from_numpy(state)\n",
    "next_state = torch.from_numpy(next_state)\n",
    "action = torch.LongTensor(action)\n",
    "reward = torch.FloatTensor(reward).unsqueeze(1)\n",
    "terminated = torch.FloatTensor(terminated).unsqueeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 10, 10])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2652, 0.1641, 0.2825, 0.3029, 0.2577, 0.2576]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "my_agent.policy_model(state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Could not infer dtype of NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [94], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m is_terminated \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor([[\u001b[39mNone\u001b[39;49;00m]], device\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      2\u001b[0m is_terminated\n\u001b[0;32m      4\u001b[0m \u001b[39mif\u001b[39;00m is_terminated:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Could not infer dtype of NoneType"
     ]
    }
   ],
   "source": [
    "\n",
    "is_terminated = torch.tensor([[None]], device=\"cpu\")\n",
    "is_terminated\n",
    "\n",
    "if is_terminated:\n",
    "    print(\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30291375517845154"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_agent.policy_model(state).max(1)[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act = torch.tensor([[random.randrange(6)]], device=\"cpu\")\n",
    "act = act.squeeze(0)\n",
    "act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16414308547973633"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "torch.index_select(my_agent.policy_model(state), 1, act).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'torch.return_types.max' object has no attribute 'item'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [90], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m my_agent\u001b[39m.\u001b[39;49mpolicy_model(state)\u001b[39m.\u001b[39;49mmax(\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39;49mitem()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'torch.return_types.max' object has no attribute 'item'"
     ]
    }
   ],
   "source": [
    "my_agent.policy_model(state).max(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of my_dqn failed: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Marius\\anaconda3\\envs\\general\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 261, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"c:\\Users\\Marius\\anaconda3\\envs\\general\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 459, in superreload\n",
      "    module = reload(module)\n",
      "  File \"c:\\Users\\Marius\\anaconda3\\envs\\general\\lib\\importlib\\__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 619, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 879, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1017, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 947, in source_to_code\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"d:\\Facultate\\PhD\\work\\learning_stuff\\dqn_clean\\minatar_work\\my_dqn.py\", line 274\n",
      "    def get_action_from_model(self, state):\n",
      "    ^^^\n",
      "IndentationError: expected an indented block after function definition on line 271\n",
      "]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[3]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_agent.policy_model(state).max(1)[1].view(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:29:51) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3d388b98505db6affdad078afd4813a1fd144c004d526877f03501f5537d2bd2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
