{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Facultate\\\\PhD\\\\work\\\\learning_stuff\\\\dqn_clean\\\\minatar_work'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "os.path.dirname(os.path.abspath(\"__file__\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marius\\anaconda3\\envs\\general\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import my_dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1]\n",
    "\n",
    "a[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from minatar import Environment\n",
    "\n",
    "game = \"breakout\"\n",
    "env = Environment(game)\n",
    "\n",
    "my_agent = my_dqn.AgentDQN(env, \"file_name\", False, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Avg return: 0.0 | Time per frame: 0.00021054479810926648\n"
     ]
    }
   ],
   "source": [
    "my_agent.train(10, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 10, 10])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_state = my_dqn.get_state(my_agent.env.state())\n",
    "\n",
    "current_state.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = my_agent.replay_buffer.sample(my_agent.batch_size)\n",
    "\n",
    "state, action, reward, next_state, terminated = sample\n",
    "\n",
    "state = torch.from_numpy(state)\n",
    "next_state = torch.from_numpy(next_state)\n",
    "action = torch.LongTensor(action).unsqueeze(1)\n",
    "reward = torch.FloatTensor(reward).unsqueeze(1)\n",
    "terminated = torch.FloatTensor(terminated).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 4, 10, 10])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_value = my_agent.policy_model(state).gather(1, action)\n",
    "\n",
    "next_q_values = my_agent.target_model(next_state).detach()\n",
    "next_q_value = next_q_values.max(1)[0].unsqueeze(1)\n",
    "\n",
    "expected_q_value = reward + my_agent.gamma * next_q_value * (1 - terminated)\n",
    "# expected_q_value = torch.unsqueeze(expected_q_value, 1)\n",
    "\n",
    "loss = (q_value - expected_q_value).pow(2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0969, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0438, -0.0587, -0.0161,  0.0629, -0.0603,  0.0530],\n",
       "        [-0.0463, -0.0590, -0.0196,  0.0600, -0.0618,  0.0527],\n",
       "        [-0.0438, -0.0587, -0.0161,  0.0629, -0.0603,  0.0530],\n",
       "        [-0.0438, -0.0587, -0.0161,  0.0629, -0.0603,  0.0530],\n",
       "        [-0.0438, -0.0587, -0.0161,  0.0629, -0.0603,  0.0530],\n",
       "        [-0.0438, -0.0587, -0.0161,  0.0629, -0.0603,  0.0530],\n",
       "        [-0.0438, -0.0587, -0.0161,  0.0629, -0.0603,  0.0530],\n",
       "        [-0.0438, -0.0587, -0.0161,  0.0629, -0.0603,  0.0530],\n",
       "        [-0.0438, -0.0587, -0.0161,  0.0629, -0.0603,  0.0530],\n",
       "        [-0.0438, -0.0587, -0.0161,  0.0629, -0.0603,  0.0530],\n",
       "        [-0.0438, -0.0587, -0.0161,  0.0629, -0.0603,  0.0530],\n",
       "        [-0.0438, -0.0587, -0.0161,  0.0629, -0.0603,  0.0530],\n",
       "        [-0.0438, -0.0587, -0.0161,  0.0629, -0.0603,  0.0530],\n",
       "        [-0.0438, -0.0587, -0.0161,  0.0629, -0.0603,  0.0530],\n",
       "        [-0.0438, -0.0587, -0.0161,  0.0629, -0.0603,  0.0530],\n",
       "        [-0.0438, -0.0587, -0.0161,  0.0629, -0.0603,  0.0530],\n",
       "        [-0.0438, -0.0587, -0.0161,  0.0629, -0.0603,  0.0530],\n",
       "        [-0.0438, -0.0587, -0.0161,  0.0629, -0.0603,  0.0530],\n",
       "        [-0.0438, -0.0587, -0.0161,  0.0629, -0.0603,  0.0530],\n",
       "        [-0.0438, -0.0587, -0.0161,  0.0629, -0.0603,  0.0530],\n",
       "        [-0.0438, -0.0587, -0.0161,  0.0629, -0.0603,  0.0530],\n",
       "        [-0.0438, -0.0587, -0.0161,  0.0629, -0.0603,  0.0530],\n",
       "        [-0.0438, -0.0587, -0.0161,  0.0629, -0.0603,  0.0530],\n",
       "        [-0.0438, -0.0587, -0.0161,  0.0629, -0.0603,  0.0530],\n",
       "        [-0.0438, -0.0587, -0.0161,  0.0629, -0.0603,  0.0530],\n",
       "        [-0.0438, -0.0587, -0.0161,  0.0629, -0.0603,  0.0530],\n",
       "        [-0.0438, -0.0587, -0.0161,  0.0629, -0.0603,  0.0530],\n",
       "        [-0.0438, -0.0587, -0.0161,  0.0629, -0.0603,  0.0530],\n",
       "        [-0.0438, -0.0587, -0.0161,  0.0629, -0.0603,  0.0530],\n",
       "        [-0.0438, -0.0587, -0.0161,  0.0629, -0.0603,  0.0530],\n",
       "        [-0.0438, -0.0587, -0.0161,  0.0629, -0.0603,  0.0530],\n",
       "        [-0.0438, -0.0587, -0.0161,  0.0629, -0.0603,  0.0530]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_value = my_agent.policy_model(state)\n",
    "q_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_value.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action.unsqueeze(0).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_value = my_agent.policy_model(state).gather(1, action.unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_value.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_q_values = my_agent.target_model(next_state).detach()\n",
    "next_q_value = next_q_values.max(1)[0].unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_q_value.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terminated.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward.unsqueeze(1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0429],\n",
       "        [0.0429],\n",
       "        [0.0429],\n",
       "        [0.0429],\n",
       "        [0.0429],\n",
       "        [0.0429],\n",
       "        [0.0429],\n",
       "        [0.0429],\n",
       "        [0.0429],\n",
       "        [0.0429],\n",
       "        [0.0429],\n",
       "        [0.0429],\n",
       "        [0.0429],\n",
       "        [0.0429],\n",
       "        [0.0429],\n",
       "        [0.0429],\n",
       "        [0.0429],\n",
       "        [0.0429],\n",
       "        [0.0429],\n",
       "        [0.0429],\n",
       "        [0.0429],\n",
       "        [0.0429],\n",
       "        [0.0429],\n",
       "        [0.0429],\n",
       "        [0.0429],\n",
       "        [0.0429],\n",
       "        [0.0429],\n",
       "        [0.0429],\n",
       "        [0.0429],\n",
       "        [0.0429],\n",
       "        [0.0429],\n",
       "        [0.0429]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_q_value = reward.unsqueeze(1) + my_agent.gamma * next_q_value * (1 - terminated.unsqueeze(1))\n",
    "# expected_q_value = torch.unsqueeze(expected_q_value, 1)\n",
    "expected_q_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_q_value.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = (q_value - expected_q_value).pow(2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0049, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, action, reward, next_state, terminated = sample\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, action, reward, next_state, terminated = sample\n",
    "\n",
    "state = torch.from_numpy(state)\n",
    "next_state = torch.from_numpy(next_state)\n",
    "action = torch.LongTensor(action).unsqueeze(1)\n",
    "reward = torch.FloatTensor(reward).unsqueeze(1)\n",
    "terminated = torch.FloatTensor(terminated).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_value = my_agent.policy_model(state).gather(1, action)\n",
    "q_value.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_q_values = my_agent.target_model(next_state).detach()\n",
    "next_q_values = next_q_values.max(1)[0].unsqueeze(1)\n",
    "expected_q_value = reward + my_agent.gamma * next_q_values * (1 - terminated)\n",
    "expected_q_value.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0024, grad_fn=<SmoothL1LossBackward0>)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = f.smooth_l1_loss(expected_q_value, q_value)\n",
    "loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_s_prime_a_prime = torch.zeros(len(state), 1)\n",
    "if len(none_terminal_next_states) != 0:\n",
    "    Q_s_prime_a_prime[none_terminal_next_state_index] = my_agent.target_model(none_terminal_next_states).detach().max(1)[0].unsqueeze(1)\n",
    "Q_s_prime_a_prime.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0429],\n",
       "        [0.0429],\n",
       "        [0.0429],\n",
       "        [0.0429],\n",
       "        [0.0429],\n",
       "        [0.0429],\n",
       "        [0.0429],\n",
       "        [0.0429],\n",
       "        [0.0429],\n",
       "        [0.0429],\n",
       "        [0.0429],\n",
       "        [0.0429],\n",
       "        [0.0429],\n",
       "        [0.0429],\n",
       "        [0.0429],\n",
       "        [0.0429],\n",
       "        [0.0429],\n",
       "        [0.0429],\n",
       "        [0.0429],\n",
       "        [0.0429],\n",
       "        [0.0429],\n",
       "        [0.0429],\n",
       "        [0.0429],\n",
       "        [0.0429],\n",
       "        [0.0429],\n",
       "        [0.0429],\n",
       "        [0.0429],\n",
       "        [0.0429],\n",
       "        [0.0429],\n",
       "        [0.0429],\n",
       "        [0.0429],\n",
       "        [0.0429]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = reward + 0.99 * Q_s_prime_a_prime\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as f\n",
    "\n",
    "loss = f.smooth_l1_loss(target, Q_s_a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0024, grad_fn=<SmoothL1LossBackward0>)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3d388b98505db6affdad078afd4813a1fd144c004d526877f03501f5537d2bd2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
